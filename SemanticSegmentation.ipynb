{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check keras backend to see if GPU is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.backend.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data pre-process\n",
    "### data visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes in this label image:12,image hight:360,image width:480\n",
      "minimum label img = 0, maximum label img = 11, Total number of label img classes = 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "training_image_path = 'dataset/images_prepped_train'\n",
    "training_label_path = 'dataset/annotations_prepped_train'\n",
    "trainingFileNameList = os.listdir(training_image_path)\n",
    "# img_training = mpimg.imread(os.path.join(training_image_path,trainingFileNameList[0]))\n",
    "img_training = cv2.imread(os.path.join(training_image_path,trainingFileNameList[10]))\n",
    "labelingFileNameList = os.listdir(training_label_path)\n",
    "img_labeling = cv2.imread(os.path.join(training_label_path,labelingFileNameList[10]))\n",
    "\n",
    "rows, cols, chanels = img_labeling.shape\n",
    "\n",
    "mi, ma = np.min(img_labeling), np.max(img_labeling)\n",
    "n_classes = ma - mi + 1\n",
    "print('Number of Classes in this label image:{},image hight:{},image width:{}'.format(n_classes, rows, cols))\n",
    "print('minimum label img = {}, maximum label img = {}, Total number of label img classes = {}'.format(mi, ma, n_classes))\n",
    "img_training = cv2.resize(img_training,((cols//32)*32, (rows//32)*32))\n",
    "img_labeling = cv2.resize(img_labeling,((cols//32)*32, (rows//32)*32))\n",
    "# Plotting\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.set_title('training BGR')\n",
    "ax1.imshow(img_training)\n",
    "\n",
    "ax2.set_title('labeling')\n",
    "ax2.imshow(img_labeling)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "for k in range(mi,ma+1):\n",
    "    ax = fig.add_subplot(3,n_classes/3,k+1)\n",
    "    ax.imshow((img_labeling == k)*1.0)\n",
    "    ax.set_title(\"label = {}\".format(k-mi))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "def generator(samples, batch_size = 32):\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "#         shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            labels = []\n",
    "            for batch_sample in batch_samples:\n",
    "                trainPath = os.path.join(training_image_path, batch_sample)\n",
    "                labelPath = os.path.join(training_label_path, batch_sample)\n",
    "                img_train = cv2.imread(trainPath)\n",
    "                img_train = cv2.resize(img_train, ((cols//32)*32, (rows//32)*32))\n",
    "                \n",
    "                img_label = cv2.imread(labelPath)\n",
    "                img_label = cv2.resize(img_label, ((cols//32)*32, (rows//32)*32))\n",
    "                seg_labels = np.zeros(((rows//32)*32 , (cols//32)*32  , n_classes ))\n",
    "                img_label = img_label[:, : , 0]\n",
    "\n",
    "                for c in range(n_classes):\n",
    "                    seg_labels[: , : , c ] = (img_label == c ).astype(int)\n",
    "        \n",
    "                images.append(img_train)\n",
    "                labels.append(seg_labels)\n",
    "\n",
    "            x = np.array(images)\n",
    "            y = np.array(labels)\n",
    "            yield sklearn.utils.shuffle(x, y)\n",
    "            \n",
    "\n",
    "# train_generator = generator(trainingFileNameList, batch_size=32)\n",
    "train_samples, validation_samples = train_test_split(trainingFileNameList, test_size=0.2)\n",
    "train_generator = generator(train_samples, batch_size=8)\n",
    "validation_generator = generator(validation_samples, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# trainingImgs = []\n",
    "# labelImgs = []\n",
    "# for file in tqdm(trainingFileNameList):\n",
    "#     path = os.path.join(training_image_path, file)\n",
    "#     img = cv2.imread(path, 1)\n",
    "#     img_resize = cv2.resize(img, ((cols//32)*32, (rows//32)*32))\n",
    "#     trainingImgs.append(img_resize)\n",
    "    \n",
    "# for file in tqdm(labelingFileNameList):\n",
    "#     path = os.path.join(training_label_path, file)\n",
    "#     img = cv2.imread(path, 1)\n",
    "#     img_resize = cv2.resize(img, ((cols//32)*32, (rows//32)*32))\n",
    "    \n",
    "    \n",
    "#     seg_labels = np.zeros(((rows//32)*32 , (cols//32)*32  , n_classes ))\n",
    "#     img_resize = img_resize[:, : , 0]\n",
    "    \n",
    "#     for c in range(n_classes):\n",
    "#         seg_labels[: , : , c ] = (img_resize == c ).astype(int)\n",
    "#     ##seg_labels = np.reshape(seg_labels, ( width*height,nClasses  ))\n",
    "#     labelImgs.append(seg_labels)\n",
    "    \n",
    "# assert len(trainingImgs) == len(labelImgs), 'training set and labeling set should be same size.'\n",
    "# trainingImgs = np.array(trainingImgs)\n",
    "# labelImgs = np.array(labelImgs)\n",
    "# print('training image set size:{}, label image set size:{}'.format(trainingImgs.shape, labelImgs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras and Tensorflow to develop deep learning FCN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of VGG weights\n",
    "VGG_Weights_path = \"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 352, 480, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 352, 480, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 352, 480, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 176, 240, 64) 0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 176, 240, 128 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 176, 240, 128 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 88, 120, 128) 0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 88, 120, 256) 295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 88, 120, 256) 590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 88, 120, 256) 590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 44, 60, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 44, 60, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 44, 60, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 44, 60, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 22, 30, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 22, 30, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 22, 30, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 22, 30, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 11, 15, 512)  0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 11, 15, 4096) 102764544   block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool4_11 (Conv2D)               (None, 22, 30, 12)   6156        block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv7 (Conv2D)                  (None, 11, 15, 4096) 16781312    conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 44, 60, 12)   576         pool4_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool3_11 (Conv2D)               (None, 44, 60, 12)   3084        block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 44, 60, 12)   786432      conv7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 44, 60, 12)   0           conv2d_transpose_2[0][0]         \n",
      "                                                                 pool3_11[0][0]                   \n",
      "                                                                 conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 352, 480, 12) 9216        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 352, 480, 12) 0           conv2d_transpose_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 135,066,008\n",
      "Trainable params: 135,066,008\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "def FCN8( nClasses, input_height=224, input_width=224):\n",
    "    ## input_height and width must be devisible by 32 because maxpooling with filter size = (2,2) is operated 5 times,\n",
    "    ## which makes the input_height and width 2^5 = 32 times smaller\n",
    "    assert input_height%32 == 0, 'input_height must be devisible by 32 because maxpooling with filter size = (2,2)'\n",
    "    assert input_width%32 == 0, 'input_width must be devisible by 32 because maxpooling with filter size = (2,2)'\n",
    "    IMAGE_ORDERING =  \"channels_last\" \n",
    "\n",
    "    img_input = Input(shape=(input_height,input_width, 3)) ## image shape\n",
    "    \n",
    "    ## Block 1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format=IMAGE_ORDERING )(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format=IMAGE_ORDERING )(x)\n",
    "    f1 = x\n",
    "    \n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format=IMAGE_ORDERING )(x)\n",
    "    f2 = x\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format=IMAGE_ORDERING )(x)\n",
    "    pool3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format=IMAGE_ORDERING )(x)## (None, 14, 14, 512) \n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format=IMAGE_ORDERING )(pool4)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    pool5 = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format=IMAGE_ORDERING )(x)## (None, 7, 7, 512)\n",
    "\n",
    "    #x = Flatten(name='flatten')(x)\n",
    "    #x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "    # <--> o = ( Conv2D( 4096 , ( 7 , 7 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING))(o)\n",
    "    # assuming that the input_height = input_width = 224 as in VGG data\n",
    "    \n",
    "    #x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "    # <--> o = ( Conv2D( 4096 , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING))(o)   \n",
    "    # assuming that the input_height = input_width = 224 as in VGG data\n",
    "    \n",
    "    #x = Dense(1000 , activation='softmax', name='predictions')(x)\n",
    "    # <--> o = ( Conv2D( nClasses ,  ( 1 , 1 ) ,kernel_initializer='he_normal' , data_format=IMAGE_ORDERING))(o)\n",
    "    # assuming that the input_height = input_width = 224 as in VGG data\n",
    "    \n",
    "    \n",
    "    vgg  = Model(  img_input , pool5  )\n",
    "    vgg.load_weights(VGG_Weights_path) ## loading VGG weights for the encoder parts of FCN8\n",
    "    \n",
    "    n = 4096\n",
    "    o = ( Conv2D( n , ( 7 , 7 ) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING))(pool5)\n",
    "    conv7 = ( Conv2D( n , ( 1 , 1 ) , activation='relu' , padding='same', name=\"conv7\", data_format=IMAGE_ORDERING))(o)\n",
    "    \n",
    "    \n",
    "    ## 4 times upsamping for pool4 layer\n",
    "    conv7_4 = Conv2DTranspose( nClasses , kernel_size=(4,4) ,  strides=(4,4) , use_bias=False, data_format=IMAGE_ORDERING )(conv7)\n",
    "    ## (None, 224, 224, 10)\n",
    "    ## 2 times upsampling for pool411\n",
    "    pool411 = ( Conv2D( nClasses , ( 1 , 1 ) , activation='relu' , padding='same', name=\"pool4_11\", data_format=IMAGE_ORDERING))(pool4)\n",
    "    pool411_2 = (Conv2DTranspose( nClasses , kernel_size=(2,2) ,  strides=(2,2) , use_bias=False, data_format=IMAGE_ORDERING ))(pool411)\n",
    "    \n",
    "    pool311 = ( Conv2D( nClasses , ( 1 , 1 ) , activation='relu' , padding='same', name=\"pool3_11\", data_format=IMAGE_ORDERING))(pool3)\n",
    "        \n",
    "    o = Add(name=\"add\")([pool411_2, pool311, conv7_4 ])\n",
    "    o = Conv2DTranspose( nClasses , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False, data_format=IMAGE_ORDERING )(o)\n",
    "    o = (Activation('softmax'))(o)\n",
    "    \n",
    "    model = Model(img_input, o)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = FCN8(nClasses = n_classes,  \n",
    "             input_height = (rows//32)*32, \n",
    "             input_width  = (cols//32)*32)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split between training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qwang97\\AppData\\Local\\Continuum\\miniconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\qwang97\\AppData\\Local\\Continuum\\miniconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=3, steps_per_epoch=293, validation_data=<generator..., validation_steps=74)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "293/293 [==============================] - 405s 1s/step - loss: 13.9639 - acc: 0.1309 - val_loss: 13.9702 - val_acc: 0.1333\n",
      "Epoch 2/3\n",
      "293/293 [==============================] - 391s 1s/step - loss: 13.9610 - acc: 0.1338 - val_loss: 13.9704 - val_acc: 0.1332\n",
      "Epoch 3/3\n",
      "293/293 [==============================] - 391s 1s/step - loss: 13.9609 - acc: 0.1338 - val_loss: 13.9729 - val_acc: 0.1331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b0ea0f9eb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=1E-2, decay=5**(-4), momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.fit(trainingImgs, labelImgs, validation_split=0.1, shuffle=True, batch_size=32, nb_epoch=5)\n",
    "\n",
    "model.fit_generator(train_generator, \n",
    "                    samples_per_epoch=len(train_samples), \n",
    "                    validation_data=validation_generator,\n",
    "                    nb_val_samples=len(validation_samples), \n",
    "                    nb_epoch=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
